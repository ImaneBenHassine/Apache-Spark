{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac0b692-bef8-46f1-a848-9042b7f5edea",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Introduction to PySpark\n",
    "PySpark was introduced to support Spark with Python Language. The PySpark API mostly contains the functionalities of Scikit-learn and Pandas Libraries of Python. In fact, the latest version of PySpark has computational power matching to Spark written in Scala. Dataframes in PySpark can be created primarily in two ways:\n",
    "- From an existing Resilient Distributed Dataset (RDD), which is a fundamental data structure in Spark\n",
    "- From external file sources, such as CSV, TXT, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a1b5e-31e8-436f-9527-949da4c1a4fd",
   "metadata": {},
   "source": [
    "First, we will install the pyspark library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13dff1d-c1e7-410e-a07e-a3c3685c6ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\monpc\\desktop\\01-imenbh\\projects\\pyspark\\pyenv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\monpc\\desktop\\01-imenbh\\projects\\pyspark\\pyenv\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c33541-95de-4563-ac34-520aa8341366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99487138-a98e-4326-9905-add75f5c67d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52cdd6a-7c63-42cd-87d4-73b3ee28e84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('datasets/EmployeeSampleData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d93accc-eda1-4c48-b8e9-f08d47172e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Department</th>\n",
       "      <th>Business Unit</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Age</th>\n",
       "      <th>Hire Date</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E02002</td>\n",
       "      <td>Kai Le</td>\n",
       "      <td>Controls Engineer</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>47</td>\n",
       "      <td>2/5/2022</td>\n",
       "      <td>$92,368</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E02003</td>\n",
       "      <td>Robert Patel</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>10/23/2013</td>\n",
       "      <td>$45,703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E02004</td>\n",
       "      <td>Cameron Lo</td>\n",
       "      <td>Network Administrator</td>\n",
       "      <td>IT</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>34</td>\n",
       "      <td>3/24/2019</td>\n",
       "      <td>$83,576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Employee ID     Full Name              Job Title   Department  \\\n",
       "0      E02002        Kai Le      Controls Engineer  Engineering   \n",
       "1      E02003  Robert Patel                Analyst        Sales   \n",
       "2      E02004    Cameron Lo  Network Administrator           IT   \n",
       "\n",
       "            Business Unit Gender Ethnicity  Age   Hire Date Annual Salary  \\\n",
       "0           Manufacturing   Male     Asian   47    2/5/2022      $92,368    \n",
       "1               Corporate   Male     Asian   58  10/23/2013      $45,703    \n",
       "2  Research & Development   Male     Asian   34   3/24/2019      $83,576    \n",
       "\n",
       "   Unnamed: 10  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6744e2c-43a0-478e-8839-f93884e98913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42811077-4c60-4044-8e97-f860dad1d10a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create PySpark DataFrame From an Existing RDD\n",
    "\n",
    "First create an RDD using the .parallelize() method and then convert it into a PySpark DataFrame using the .createDatFrame() method of SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2ff16-5697-4e17-8494-6cd0b2c8f7db",
   "metadata": {},
   "source": [
    "#### Importing the Libraries\n",
    "To start using PySpark, we first need to create a Spark Session. A spark session can be created by importing a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f299d6c-7f63-42b1-9e8a-0da535c6a62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20359a2-3bf6-4ff0-bd98-e23a26292e86",
   "metadata": {},
   "source": [
    "#### Creating a SparkContext\n",
    "Using the .getOrCreate() method of SparkContext to create a SparkContext for our exercise. The .getOrCreate() method will create and instantiate SparkContext into our variable sc or will fetch the old one if already created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e814ef-1462-4a95-84a6-693c05522701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaf357-1404-493c-8bb3-c52aa55d551a",
   "metadata": {},
   "source": [
    "#### Create a SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff3cb1-ff52-4ce2-bff8-c01abb226b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "SparkSession is an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD (Resilient Data Structure), DataFrame. Itâ€™s object spark is default available in pyspark-shell and it can be created programmatically using SparkSession.\n",
    "You can create as many SparkSession as you want in a PySpark application using either SparkSession.builder() or SparkSession.newSession(). Many Spark session objects are required when you wanted to keep PySpark tables (relational entities) logically separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5188a7-d773-4308-b771-8713554bbc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529025c-acec-413b-9ab0-9c8d1ff6d139",
   "metadata": {},
   "source": [
    "Given the name to the Application by passing a string to .appName() as an argument. Next, used .getOrCreate() which will create and instantiate SparkSession into the object spark. Using the .getOrCreate() method would use an existing SparkSession if one is already present else will create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51220a5-8980-40bf-9273-70a1ee5f2141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Projects').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b336d5-b729-4b7a-9414-0e95b8772e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-O58PAUC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Projects</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2d0c0843e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f53e9d-1310-4e7b-a1a7-d6d0dca951a5",
   "metadata": {},
   "source": [
    "SparkSession also includes all the APIs available in different contexts :\n",
    "\n",
    "- SparkContext,\n",
    "- SQLContext,\n",
    "- StreamingContext,\n",
    "- HiveContext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e05e5-57c8-4976-9f9f-815ede60b6e6",
   "metadata": {},
   "source": [
    "#### Creating a Resilient Data Structure (RDD)\n",
    "Using the .parallelize() method of SparkContext sc which took the tuples of marks of students.Then converting this RDD into a PySpark Dataframe. Passed numSlices value to 4 which is the number of partitions the data would parallelize into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1777a222-73c4-4594-917c-48dafc8aeedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb49541e-48d1-4885-bb4e-d8a66762df15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f2476-b113-4075-88be-8972baf02f14",
   "metadata": {},
   "source": [
    "To verify if the RDD creation is successful by checking the datatype of the variable rdd. On executing this, it gets pyspark.rdd.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e034b85-b75d-4091-8343-223122e0957d",
   "metadata": {},
   "source": [
    "#### Converting the RDD into PySpark DataFrame\n",
    "Here, The .createDataFrame() method from SparkSession spark takes data as an RDD, a Python list or a Pandas DataFrame. Here passing the RDD as data. Also created a list of strings sub which will be passed into schema attribute of .createDataFrame() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8881399-8baf-48de-bd5c-c8cbe23ef928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = ['Division','English','Mathematics','Physics','Chemistry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb48d88-5142-4e2c-b476-1a2922243243",
   "metadata": {
    "tags": []
   },
   "source": [
    "marks_df = spark.createDataFrame(rdd, schema=sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1899c1f-1614-4b0b-b18f-f9ae1dd64b98",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(type(marks_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a9c41-84d9-45ee-8ada-843cd92cceaf",
   "metadata": {},
   "source": [
    "Using the .read() methods of SparkSession to import our external Files. This will return a Spark Dataframe object. The external files format that can be imported includes JSON, TXT or CSV. The methods to import each of this file type is almost same and one can import them with no efforts. Unlike the previous method of creating PySpark Dataframe from RDD, this method is quite easier and requires only Spark Session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5a79e-6a08-4031-92e9-503a4673819c",
   "metadata": {},
   "source": [
    "### Create PySpark DataFrame From an External File\n",
    "Use option to get the head colomns\n",
    "Using the .read() methods of SparkSession to import the external Files. This will return a Spark Dataframe object. The external files format that can be imported includes JSON, TXT or CSV. The methods to import each of this file type is almost same and one can import them with no efforts. Unlike the previous method of creating PySpark Dataframe from RDD, this method is quite easier and requires only Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "133410d8-7b13-4530-9f1b-a230a60b4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark= spark.read.csv('datasets/EmployeeSampleData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9159fd8-cbed-4db7-bcea-8e9a81a62075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd4915-c47c-4ddf-9f94-2f7c311ee068",
   "metadata": {},
   "source": [
    "Use the .show() to view the contents of the file method on the PySpark Dataframe object. This will display the top 20 rows of our PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a2f1d95-b8ab-4f0d-8fad-866f71930011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------------+-----------+--------------------+------+---------+---+----------+-------------+----+\n",
      "|        _c0|         _c1|                 _c2|        _c3|                 _c4|   _c5|      _c6|_c7|       _c8|          _c9|_c10|\n",
      "+-----------+------------+--------------------+-----------+--------------------+------+---------+---+----------+-------------+----+\n",
      "|Employee ID|   Full Name|           Job Title| Department|       Business Unit|Gender|Ethnicity|Age| Hire Date|Annual Salary|null|\n",
      "|     E02002|      Kai Le|   Controls Engineer|Engineering|       Manufacturing|  Male|    Asian| 47|  2/5/2022|     $92,368 |null|\n",
      "|     E02003|Robert Patel|             Analyst|      Sales|           Corporate|  Male|    Asian| 58|10/23/2013|     $45,703 |null|\n",
      "|     E02004|  Cameron Lo|Network Administr...|         IT|Research & Develo...|  Male|    Asian| 34| 3/24/2019|     $83,576 |null|\n",
      "+-----------+------------+--------------------+-----------+--------------------+------+---------+---+----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa37382-10ad-4d47-baf4-35c3473b33d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Employee ID: string, Full Name: string, Job Title: string, Department: string, Business Unit: string, Gender: string, Ethnicity: string, Age: string, Hire Date: string, Annual Salary: string, _c10: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('datasets/EmployeeSampleData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d3e20ab-b5b8-4298-b48f-ab0713939a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------------+-----------+--------------------+------+---------+---+----------+-------------+----+\n",
      "|Employee ID|   Full Name|           Job Title| Department|       Business Unit|Gender|Ethnicity|Age| Hire Date|Annual Salary|_c10|\n",
      "+-----------+------------+--------------------+-----------+--------------------+------+---------+---+----------+-------------+----+\n",
      "|     E02002|      Kai Le|   Controls Engineer|Engineering|       Manufacturing|  Male|    Asian| 47|  2/5/2022|     $92,368 |null|\n",
      "|     E02003|Robert Patel|             Analyst|      Sales|           Corporate|  Male|    Asian| 58|10/23/2013|     $45,703 |null|\n",
      "|     E02004|  Cameron Lo|Network Administr...|         IT|Research & Develo...|  Male|    Asian| 34| 3/24/2019|     $83,576 |null|\n",
      "+-----------+------------+--------------------+-----------+--------------------+------+---------+---+----------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('datasets/EmployeeSampleData.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58b624fc-e177-49fd-8b47-fb6b51e7ad8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('datasets/EmployeeSampleData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01734eab-77bd-40f4-b407-9f9d2e35e041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "196ce3db-9a0a-40b4-b44a-acbd21709d74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Employee ID='E02002', Full Name='Kai Le', Job Title='Controls Engineer', Department='Engineering', Business Unit='Manufacturing', Gender='Male', Ethnicity='Asian', Age='47', Hire Date='2/5/2022', Annual Salary='$92,368 ', _c10=None),\n",
       " Row(Employee ID='E02003', Full Name='Robert Patel', Job Title='Analyst', Department='Sales', Business Unit='Corporate', Gender='Male', Ethnicity='Asian', Age='58', Hire Date='10/23/2013', Annual Salary='$45,703 ', _c10=None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f70c34-b51f-4ced-a3f0-01600142266a",
   "metadata": {},
   "source": [
    "#### Schema of PySpark DataFrame\n",
    "useful when we have tens or hundreds of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14f19509-2a84-446a-a286-2719c410bf25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee ID: string (nullable = true)\n",
      " |-- Full Name: string (nullable = true)\n",
      " |-- Job Title: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Business Unit: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Hire Date: string (nullable = true)\n",
      " |-- Annual Salary: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88cd3cc-435b-4a3f-be8f-7ffe75fec10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
