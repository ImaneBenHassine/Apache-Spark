{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb97442-9910-4a06-a3e5-f12a3d6df3c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark ETL with SQL Database MySQL\n",
    "\n",
    "1. Install required spark libraries\n",
    "2. Create connection with MySQL Database\n",
    "3. Read data from MySQL Database\n",
    "4. Transform data\n",
    "5. Write data into MySQL Server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005ace9-9973-4081-bbad-ff54e2e8ca43",
   "metadata": {},
   "source": [
    "### 1- Spark Librairies\n",
    "\n",
    "Start Spark Session and Load all the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1c1f11-5758-49b9-bba3-2df3402f61e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82d7c378-8240-4693-b552-bf9ad16feff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "           .appName('JDBC Cnx') \\\n",
    "           .config(\"spark.jars\", \"mysql-connector-java-8.0.13.jar\")\\\n",
    "           .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4ea1d14-a0e4-40e6-87ae-9f075d078d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-O58PAUC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JDBC Cnx</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2573b62a750>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016246f5-3a39-478b-906e-1f2c937f8fb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2- Create Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575fe1c6-8381-44b1-8be5-df85b1c40b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load CSV file into DataFrame\n",
    "mysql_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\",\"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://127.0.0.1:3306/spark_db\") \\\n",
    "    .option(\"dbtable\", \"orders\") \\\n",
    "    .option(\"user\", \"analystdata\") \\\n",
    "    .option(\"password\", \"pwd#90\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d2263d-a2ee-478e-960e-913b77c04eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row_ID: integer (nullable = true)\n",
      " |-- Order_Priority: string (nullable = true)\n",
      " |-- Discount: decimal(10,2) (nullable = true)\n",
      " |-- Unit_Price: decimal(10,2) (nullable = true)\n",
      " |-- Shipping_Cost: decimal(10,2) (nullable = true)\n",
      " |-- Customer_ID: integer (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Ship_Mode: string (nullable = true)\n",
      " |-- Customer_Segment: string (nullable = true)\n",
      " |-- Product_Category: string (nullable = true)\n",
      " |-- Product_Sub_Category: string (nullable = true)\n",
      " |-- Product_Container: string (nullable = true)\n",
      " |-- Product_Name: string (nullable = true)\n",
      " |-- Product_Base_Margin: decimal(10,2) (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- State_or_Province: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Postal_Code: string (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Ship_Date: string (nullable = true)\n",
      " |-- Profit: decimal(10,2) (nullable = true)\n",
      " |-- Quantity_ordered_new: integer (nullable = true)\n",
      " |-- Sales: decimal(10,2) (nullable = true)\n",
      " |-- Order_ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mysql_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7381d7f6-b677-4d5c-924e-c71428dbd2af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+----------+-------------+-----------+---------------+-----------+----------------+----------------+--------------------+-----------------+--------------------+-------------------+-------+-----------------+---------+-----------+----------+---------+------+--------------------+-----+--------+\n",
      "|Row_ID|Order_Priority|Discount|Unit_Price|Shipping_Cost|Customer_ID|  Customer_Name|  Ship_Mode|Customer_Segment|Product_Category|Product_Sub_Category|Product_Container|        Product_Name|Product_Base_Margin| Region|State_or_Province|     City|Postal_Code|Order_Date|Ship_Date|Profit|Quantity_ordered_new|Sales|Order_ID|\n",
      "+------+--------------+--------+----------+-------------+-----------+---------------+-----------+----------------+----------------+--------------------+-----------------+--------------------+-------------------+-------+-----------------+---------+-----------+----------+---------+------+--------------------+-----+--------+\n",
      "| 18606| Not Specified|    0.01|      2.88|         0.50|          2|Janice Fletcher|Regular Air|       Corporate| Office Supplies|              Labels|        Small Box|            Avery 49|               0.36|Central|         Illinois|  Addison|      60101| 5/28/2012|5/30/2012|  1.32|                   2| 5.90|   88525|\n",
      "| 20847|          High|    0.01|      2.84|         0.93|          3|  Bonnie Potter|Express Air|       Corporate| Office Supplies| Pens & Art Supplies|         Wrap Bag|SANFORD Liquid Ac...|               0.54|   West|       Washington|Anacortes|      98221|  7/7/2010| 7/8/2010|  4.56|                   4|13.01|   88522|\n",
      "| 23086| Not Specified|    0.03|      6.68|         6.15|          3|  Bonnie Potter|Express Air|       Corporate| Office Supplies|               Paper|        Small Box|          Xerox 1968|               0.37|   West|       Washington|Anacortes|      98221| 7/27/2011|7/28/2011|-47.64|                   7|49.92|   88523|\n",
      "+------+--------------+--------+----------+-------------+-----------+---------------+-----------+----------------+----------------+--------------------+-----------------+--------------------+-------------------+-------+-----------------+---------+-----------+----------+---------+------+--------------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mysql_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1842bf-d389-42e5-a43c-f13d0959295f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2- Write DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5890839-4df2-4487-9f60-e524edb63a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version= 3.4.0\n"
     ]
    }
   ],
   "source": [
    "print( \"spark version=\" ,SparkSession.builder.appName(\"test\").getOrCreate().version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44393715-3b38-4c67-9407-b22a1ebeedef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.4.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\MonPC\\Desktop\\01-ImenBH\\Projects\\PySpark\\pyenv\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7abb176-3c60-4106-b06b-a1a24a49fa10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[9] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an RDD by passing Python list object to sparkContext.parallelize() function\n",
    "columns = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(columns)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b71da230-91dc-4d6c-9b40-dfe150ed63cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|name     |id |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting PySpark RDD to DataFrame can be done using toDF(), createDataFrame()\n",
    "\n",
    "ColumnName = [\"name\",\"id\"]\n",
    "df_test= spark.createDataFrame(rdd, schema =ColumnName)\n",
    "df_test.printSchema()\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca278f33-4cd0-4a07-b602-d00f6435cf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.write\\\n",
    "    .format('jdbc')\\\n",
    "    .option(\"driver\",\"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://127.0.0.1:3306/spark_db\") \\\n",
    "    .option(\"dbtable\", \"df_test\") \\\n",
    "    .option(\"user\", \"analystdata\") \\\n",
    "    .option(\"password\", \"pwd#90\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6af4a-70c0-48a3-83fa-89037b91d806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
